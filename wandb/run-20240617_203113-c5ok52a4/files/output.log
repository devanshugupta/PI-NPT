/Users/devu/anaconda3/envs/npt/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/Users/devu/PycharmProjects/non-parametric-transformers/npt/mask.py:108: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403207619/work/torch/csrc/utils/tensor_new.cpp:620.)
  mask = torch.sparse.FloatTensor(
Loaded metadata for fixed test set. n_cv_splits set to 1.
CV Splits for this dataset are cached. Loading from file.
CV Index: 0
Train-test Split 1/1
Building NPT.
All features are either categorical or numerical. Not going to bother doing feature type embeddings.
Using feature type embedding (unique embedding for categorical and numerical features).
Using feature index embedding (unique embedding for each column).
Clipping gradients to value 1.0.
Model has 31576838 parameters,batch size -1.
Initialized "lookahead_lamb" optimizer.
Warming up for 70000.0/100000.0 steps.
Initialized "flat_and_anneal" learning rate scheduler.
Initialized "cosine" augmentation/label tradeoff annealer. Annealing to minimum value in 100000 steps.
Disabled AUROC in loss module.
masked tensor (input to model) ------------ [tensor([[ 0.2597,  0.0000],
        [ 0.9760,  0.0000],
        [-1.6595,  0.0000],
        ...,
        [-1.1189,  0.0000],
        [-0.0106,  0.0000],
        [ 1.4626,  0.0000]], requires_grad=True), tensor([[ 1.8207,  0.0000],
        [-1.2226,  0.0000],
        [-1.2226,  0.0000],
        ...,
        [-1.0689,  0.0000],
        [-1.0074,  0.0000],
        [-0.3004,  0.0000]], requires_grad=True), tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        ...,
        [0., 1.],
        [0., 1.],
        [0., 1.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True), tensor([[0., 1.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 1.],
        [0., 0.]], requires_grad=True), tensor([[0., 1.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True)]
Computing PINNs Loss:
u_pred requires_grad, grad_fn: True,<AddmmBackward0 object at 0x134b38760>
x requires_grad, grad_fn: True,None
t requires_grad, grad_fn: True,None
x shape: torch.Size([1256, 2])
t shape: torch.Size([1256, 2])
u_sum tensor(-120.0883, grad_fn=<SumBackward0>)
u---------------------- tensor([[ 0.0216],
        [-0.0244],
        [-0.1581],
        ...,
        [-0.3150],
        [-0.1184],
        [-0.1019]], grad_fn=<AddmmBackward0>)
t---------------------- tensor([[ 1.8207,  0.0000],
        [-1.2226,  0.0000],
        [-1.2226,  0.0000],
        ...,
        [-1.0689,  0.0000],
        [-1.0074,  0.0000],
        [-0.3004,  0.0000]], requires_grad=True)
x---------------------- tensor([[ 0.2597,  0.0000],
        [ 0.9760,  0.0000],
        [-1.6595,  0.0000],
        ...,
        [-1.1189,  0.0000],
        [-0.0106,  0.0000],
        [ 1.4626,  0.0000]], requires_grad=True)
tensor([[-0.0285, -0.0048],
        [-0.0035, -0.0052],
        [-0.0116, -0.0377],
        ...,
        [-0.0372, -0.0412],
        [-0.0449, -0.0150],
        [-0.0524, -0.0330]], grad_fn=<MmBackward0>) tensor([[ 0.0030, -0.0068],
        [-0.0133, -0.0100],
        [-0.0056, -0.0075],
        ...,
        [ 0.0085,  0.0004],
        [-0.0137, -0.0030],
        [ 0.0340,  0.0123]], grad_fn=<MmBackward0>) tensor([[-0.0023, -0.0094],
        [-0.0098, -0.0134],
        [ 0.0002, -0.0088],
        ...,
        [ 0.0071, -0.0061],
        [-0.0044, -0.0081],
        [-0.0035, -0.0066]], grad_fn=<MmBackward0>)
Physics Loss:  tensor(0.0006, grad_fn=<MeanBackward0>)
Done-------------------------
masked tensor (input to model) ------------ [tensor([[ 0.2056,  0.0000],
        [ 0.0000,  1.0000],
        [-0.6864,  0.0000],
        ...,
        [-1.3351,  0.0000],
        [-1.7136,  0.0000],
        [-1.6054,  0.0000]], requires_grad=True), tensor([[-0.5771,  0.0000],
        [-0.8537,  0.0000],
        [-1.2226,  0.0000],
        ...,
        [-0.1467,  0.0000],
        [ 0.0685,  0.0000],
        [ 1.7285,  0.0000]], requires_grad=True), tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        ...,
        [0., 1.],
        [0., 1.],
        [0., 1.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True), tensor([[0., 1.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 1.],
        [0., 1.]], requires_grad=True), tensor([[0., 1.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 1.],
        [0., 0.],
        [0., 0.]], requires_grad=True)]
Computing PINNs Loss:
u_pred requires_grad, grad_fn: True,<AddmmBackward0 object at 0x316268b50>
x requires_grad, grad_fn: True,None
t requires_grad, grad_fn: True,None
x shape: torch.Size([1256, 2])
t shape: torch.Size([1256, 2])
u_sum tensor(324.8829, grad_fn=<SumBackward0>)
u---------------------- tensor([[0.0831],
        [0.2397],
        [0.1529],
        ...,
        [0.5208],
        [0.3748],
        [0.2947]], grad_fn=<AddmmBackward0>)
t---------------------- tensor([[-0.5771,  0.0000],
        [-0.8537,  0.0000],
        [-1.2226,  0.0000],
        ...,
        [-0.1467,  0.0000],
        [ 0.0685,  0.0000],
        [ 1.7285,  0.0000]], requires_grad=True)
x---------------------- tensor([[ 0.2056,  0.0000],
        [ 0.0000,  1.0000],
        [-0.6864,  0.0000],
        ...,
        [-1.3351,  0.0000],
        [-1.7136,  0.0000],
        [-1.6054,  0.0000]], requires_grad=True)
tensor([[ 0.0030, -0.0189],
        [-0.0003, -0.0107],
        [-0.0166, -0.0359],
        ...,
        [-0.0333, -0.0156],
        [-0.0340, -0.0163],
        [-0.0288, -0.0262]], grad_fn=<MmBackward0>) tensor([[-0.0203, -0.0071],
        [-0.0075, -0.0256],
        [ 0.0097,  0.0011],
        ...,
        [ 0.0049,  0.0394],
        [-0.0004, -0.0104],
        [-0.0017,  0.0008]], grad_fn=<MmBackward0>) tensor([[-0.0045, -0.0165],
        [-0.0046, -0.0119],
        [-0.0080, -0.0104],
        ...,
        [-0.0100, -0.0280],
        [-0.0066, -0.0101],
        [-0.0070, -0.0146]], grad_fn=<MmBackward0>)
Physics Loss:  tensor(0.0010, grad_fn=<MeanBackward0>)
Done-------------------------
masked tensor (input to model) ------------ [tensor([[0.4489, 0.0000],
        [1.7194, 0.0000],
        [0.0000, 1.0000],
        ...,
        [1.3815, 0.0000],
        [1.3274, 0.0000],
        [1.2058, 0.0000]], requires_grad=True), tensor([[ 0.0685,  0.0000],
        [-0.1774,  0.0000],
        [ 1.1137,  0.0000],
        ...,
        [ 1.6977,  0.0000],
        [ 0.8370,  0.0000],
        [ 0.0000,  1.0000]], requires_grad=True), tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        ...,
        [0., 1.],
        [0., 1.],
        [0., 1.]], requires_grad=True), tensor([[0., 1.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 1.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 1.],
        [0., 0.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 1.],
        ...,
        [0., 0.],
        [0., 1.],
        [0., 0.]], requires_grad=True)]
Computing PINNs Loss:
u_pred requires_grad, grad_fn: True,<AddmmBackward0 object at 0x316268ac0>
x requires_grad, grad_fn: True,None
t requires_grad, grad_fn: True,None
x shape: torch.Size([1256, 2])
t shape: torch.Size([1256, 2])
u_sum tensor(-106.9954, grad_fn=<SumBackward0>)
u---------------------- tensor([[-0.0448],
        [-0.1380],
        [-0.0097],
        ...,
        [-0.1117],
        [ 0.0482],
        [-0.1009]], grad_fn=<AddmmBackward0>)
t---------------------- tensor([[ 0.0685,  0.0000],
        [-0.1774,  0.0000],
        [ 1.1137,  0.0000],
        ...,
        [ 1.6977,  0.0000],
        [ 0.8370,  0.0000],
        [ 0.0000,  1.0000]], requires_grad=True)
x---------------------- tensor([[0.4489, 0.0000],
        [1.7194, 0.0000],
        [0.0000, 1.0000],
        ...,
        [1.3815, 0.0000],
        [1.3274, 0.0000],
        [1.2058, 0.0000]], requires_grad=True)
tensor([[-0.0368,  0.0011],
        [ 0.0084, -0.0238],
        [-0.0256, -0.0073],
        ...,
        [-0.0154,  0.0003],
        [ 0.0030,  0.0069],
        [-0.0233, -0.0372]], grad_fn=<MmBackward0>) tensor([[-0.0388,  0.0011],
        [-0.0165, -0.0182],
        [-0.0070, -0.0013],
        ...,
        [-0.0301, -0.0013],
        [-0.0428, -0.0048],
        [-0.0448, -0.0009]], grad_fn=<MmBackward0>) tensor([[-0.0067, -0.0021],
        [ 0.0026, -0.0070],
        [-0.0031, -0.0070],
        ...,
        [ 0.0054,  0.0023],
        [ 0.0032, -0.0068],
        [ 0.0061, -0.0034]], grad_fn=<MmBackward0>)
Physics Loss:  tensor(0.0014, grad_fn=<MeanBackward0>)
Done-------------------------
masked tensor (input to model) ------------ [tensor([[ 0.1110,  0.0000],
        [ 0.2597,  0.0000],
        [-1.5649,  0.0000],
        ...,
        [-1.4027,  0.0000],
        [ 1.0571,  0.0000],
        [ 0.4219,  0.0000]], requires_grad=True), tensor([[-0.7308,  0.0000],
        [-1.2226,  0.0000],
        [ 0.0685,  0.0000],
        ...,
        [ 0.9600,  0.0000],
        [ 0.0000,  1.0000],
        [ 0.9907,  0.0000]], requires_grad=True), tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        ...,
        [0., 1.],
        [0., 1.],
        [0., 1.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 1.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 1.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 1.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True)]
Computing PINNs Loss:
u_pred requires_grad, grad_fn: True,<AddmmBackward0 object at 0x316268160>
x requires_grad, grad_fn: True,None
t requires_grad, grad_fn: True,None
x shape: torch.Size([1256, 2])
t shape: torch.Size([1256, 2])
u_sum tensor(-277.0220, grad_fn=<SumBackward0>)
u---------------------- tensor([[-0.1705],
        [-0.4721],
        [-0.0608],
        ...,
        [-0.1366],
        [-0.2507],
        [-0.2732]], grad_fn=<AddmmBackward0>)
t---------------------- tensor([[-0.7308,  0.0000],
        [-1.2226,  0.0000],
        [ 0.0685,  0.0000],
        ...,
        [ 0.9600,  0.0000],
        [ 0.0000,  1.0000],
        [ 0.9907,  0.0000]], requires_grad=True)
x---------------------- tensor([[ 0.1110,  0.0000],
        [ 0.2597,  0.0000],
        [-1.5649,  0.0000],
        ...,
        [-1.4027,  0.0000],
        [ 1.0571,  0.0000],
        [ 0.4219,  0.0000]], requires_grad=True)
tensor([[-0.0026, -0.0260],
        [-0.0126, -0.0112],
        [-0.0356, -0.0258],
        ...,
        [-0.0186, -0.0028],
        [-0.0145, -0.0057],
        [-0.0138, -0.0098]], grad_fn=<MmBackward0>) tensor([[-0.0437, -0.0087],
        [-0.0495, -0.0098],
        [-0.0351, -0.0061],
        ...,
        [-0.0572, -0.0187],
        [-0.0289, -0.0148],
        [-0.0586, -0.0179]], grad_fn=<MmBackward0>) tensor([[-0.0041, -0.0026],
        [ 0.0039,  0.0039],
        [-0.0063, -0.0054],
        ...,
        [ 0.0032, -0.0031],
        [ 0.0038, -0.0032],
        [ 0.0057,  0.0026]], grad_fn=<MmBackward0>)
Physics Loss:  tensor(0.0020, grad_fn=<MeanBackward0>)
Done-------------------------
masked tensor (input to model) ------------ [tensor([[ 0.3678,  0.0000],
        [-1.2675,  0.0000],
        [ 0.6787,  0.0000],
        ...,
        [ 0.0000,  1.0000],
        [ 0.2056,  0.0000],
        [-0.9432,  0.0000]], requires_grad=True), tensor([[0.4989, 0.0000],
        [0.1607, 0.0000],
        [0.3759, 0.0000],
        ...,
        [1.0214, 0.0000],
        [0.8063, 0.0000],
        [0.0000, 1.0000]], requires_grad=True), tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        ...,
        [0., 1.],
        [0., 1.],
        [0., 1.]], requires_grad=True), tensor([[0., 1.],
        [0., 0.],
        [0., 1.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True), tensor([[0.0000, 0.0000],
        [0.0000, 1.0000],
        [0.0000, 0.0000],
        ...,
        [0.8916, 0.0000],
        [1.1720, 0.0000],
        [0.0000, 0.0000]], requires_grad=True), tensor([[0., 1.],
        [0., 0.],
        [0., 1.],
        ...,
        [0., 0.],
        [0., 1.],
        [0., 0.]], requires_grad=True)]
Computing PINNs Loss:
u_pred requires_grad, grad_fn: True,<AddmmBackward0 object at 0x316268160>
x requires_grad, grad_fn: True,None
t requires_grad, grad_fn: True,None
x shape: torch.Size([1256, 2])
t shape: torch.Size([1256, 2])
u_sum tensor(12.0365, grad_fn=<SumBackward0>)
u---------------------- tensor([[-0.1391],
        [ 0.0363],
        [-0.2471],
        ...,
        [ 0.3546],
        [-0.0665],
        [ 0.1076]], grad_fn=<AddmmBackward0>)
t---------------------- tensor([[0.4989, 0.0000],
        [0.1607, 0.0000],
        [0.3759, 0.0000],
        ...,
        [1.0214, 0.0000],
        [0.8063, 0.0000],
        [0.0000, 1.0000]], requires_grad=True)
x---------------------- tensor([[ 0.3678,  0.0000],
        [-1.2675,  0.0000],
        [ 0.6787,  0.0000],
        ...,
        [ 0.0000,  1.0000],
        [ 0.2056,  0.0000],
        [-0.9432,  0.0000]], requires_grad=True)
tensor([[ 0.0034, -0.0147],
        [ 0.0013, -0.0118],
        [ 0.0102, -0.0094],
        ...,
        [ 0.0059, -0.0044],
        [ 0.0042, -0.0161],
        [ 0.0009, -0.0209]], grad_fn=<MmBackward0>) tensor([[-0.0603,  0.0081],
        [-0.0582, -0.0220],
        [-0.0553, -0.0125],
        ...,
        [-0.0542,  0.0116],
        [-0.0573, -0.0085],
        [-0.0517, -0.0031]], grad_fn=<MmBackward0>) tensor([[-0.0053, -0.0056],
        [-0.0069, -0.0112],
        [ 0.0030, -0.0044],
        ...,
        [-0.0104, -0.0069],
        [-0.0073, -0.0056],
        [-0.0106, -0.0100]], grad_fn=<MmBackward0>)
Physics Loss:  tensor(0.0030, grad_fn=<MeanBackward0>)
Done-------------------------
Traceback (most recent call last):
  File "run.py", line 204, in <module>
    main(args)
  File "run.py", line 23, in main
    run_cv(args=args, wandb_args=wandb_args)
  File "run.py", line 117, in run_cv
    run_cv_splits(wandb_args, args, c, wandb_run)
  File "run.py", line 195, in run_cv_splits
    trainer.train_and_eval()
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 216, in train_and_eval
    if self.per_epoch_train_eval(epoch=epoch):
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 175, in per_epoch_train_eval
    early_stop = self.eval_model(
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 224, in eval_model
    val_loss = self.run_epoch(dataset_mode='val', **kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 437, in run_epoch
    self.run_batch(
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 546, in run_batch
    self.forward_and_loss(**forward_kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 611, in forward_and_loss
    self.loss.compute(**loss_kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/loss.py", line 96, in compute
    loss_dict = self.compute_loss(*args, **kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/loss.py", line 324, in compute_loss
    physics_loss = self.convection_diffusion_loss(output, ground_truth_data, data_dict, masked_tensors)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/loss.py", line 154, in convection_diffusion_loss
    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]
  File "/Users/devu/anaconda3/envs/npt/lib/python3.8/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Traceback (most recent call last):
  File "run.py", line 204, in <module>
    main(args)
  File "run.py", line 23, in main
    run_cv(args=args, wandb_args=wandb_args)
  File "run.py", line 117, in run_cv
    run_cv_splits(wandb_args, args, c, wandb_run)
  File "run.py", line 195, in run_cv_splits
    trainer.train_and_eval()
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 216, in train_and_eval
    if self.per_epoch_train_eval(epoch=epoch):
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 175, in per_epoch_train_eval
    early_stop = self.eval_model(
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 224, in eval_model
    val_loss = self.run_epoch(dataset_mode='val', **kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 437, in run_epoch
    self.run_batch(
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 546, in run_batch
    self.forward_and_loss(**forward_kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/train.py", line 611, in forward_and_loss
    self.loss.compute(**loss_kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/loss.py", line 96, in compute
    loss_dict = self.compute_loss(*args, **kwargs)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/loss.py", line 324, in compute_loss
    physics_loss = self.convection_diffusion_loss(output, ground_truth_data, data_dict, masked_tensors)
  File "/Users/devu/PycharmProjects/non-parametric-transformers/npt/loss.py", line 154, in convection_diffusion_loss
    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]
  File "/Users/devu/anaconda3/envs/npt/lib/python3.8/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
masked tensor (input to model) ------------ [tensor([[-1.5919,  0.0000],
        [-0.6323,  0.0000],
        [-1.5784,  0.0000],
        ...,
        [-1.4432,  0.0000],
        [-0.9297,  0.0000],
        [ 1.1652,  0.0000]], requires_grad=True), tensor([[ 0.0377,  0.0000],
        [-1.2226,  0.0000],
        [ 1.1751,  0.0000],
        ...,
        [ 1.7592,  0.0000],
        [ 1.4826,  0.0000],
        [-0.7615,  0.0000]], requires_grad=True), tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        ...,
        [0., 1.],
        [0., 1.],
        [0., 1.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True), tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], requires_grad=True)]
Computing PINNs Loss:
u_pred requires_grad, grad_fn: True,None
x requires_grad, grad_fn: True,None
t requires_grad, grad_fn: True,None
x shape: torch.Size([1256, 2])
t shape: torch.Size([1256, 2])
u_sum tensor(347.5460, grad_fn=<SumBackward0>)
u---------------------- tensor([[0.4649],
        [0.3538],
        [0.4618],
        ...,
        [0.4457],
        [0.3942],
        [0.1250]], requires_grad=True)
t---------------------- tensor([[ 0.0377,  0.0000],
        [-1.2226,  0.0000],
        [ 1.1751,  0.0000],
        ...,
        [ 1.7592,  0.0000],
        [ 1.4826,  0.0000],
        [-0.7615,  0.0000]], requires_grad=True)
x---------------------- tensor([[-1.5919,  0.0000],
        [-0.6323,  0.0000],
        [-1.5784,  0.0000],
        ...,
        [-1.4432,  0.0000],
        [-0.9297,  0.0000],
        [ 1.1652,  0.0000]], requires_grad=True)